{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this article we are going to look into how you can track various experiments using Tensorboard in PyTorch and use the tracking data to tune hyperparameters. First let us see what is experiment tracking and hyperparameter tuning. \n",
    "\n",
    "### Experiment Tracking\n",
    "\n",
    "Experiment tracking refers to the process of collecting and organizing data related to machine learning experiments. It involves logging and monitoring various aspects of an experiment, such as the model architecture, hyperparameters, training progress, evaluation metrics, and visualizations.\n",
    "\n",
    "Experiment tracking helps researchers and practitioners keep a record of their experiments, making it easier to reproduce and compare results. It enables better understanding of the experiment's behavior, aids in debugging, and facilitates collaboration within teams.\n",
    "\n",
    "One popular tool for experiment tracking is TensorBoard, which is a web-based visualization tool provided by TensorFlow. While originally developed for TensorFlow, it can also be used with PyTorch, another popular deep learning framework.\n",
    "\n",
    "TensorBoard provides a user-friendly interface to explore and analyze experiment data. It allows visualizing metrics like loss and accuracy over time, examining the model graph, visualizing histograms of weights and biases, and displaying images and embeddings. It also supports interactive visualizations such as 3D embeddings and custom plots.\n",
    "\n",
    "By using TensorBoard, you can gain insights into your experiments, compare different models or hyperparameter settings, and make informed decisions for improving your machine learning models.\n",
    "\n",
    "### Hyper Parameter Tuning\n",
    "Hyperparameter tuning, also known as hyperparameter optimization, is the process of finding the optimal values for the hyperparameters of a machine learning model. Hyperparameters are configuration settings that are not learned from the data but are set prior to training and affect the learning process.\n",
    "\n",
    "Examples of hyperparameters include learning rate, batch size, number of hidden layers, number of units in each layer, regularization parameters, and activation functions. The choice of hyperparameters can significantly impact the performance and generalization ability of a model.\n",
    "\n",
    "Hyperparameter tuning is crucial because selecting appropriate values for hyperparameters can lead to better model performance. It involves systematically searching through a predefined space of possible hyperparameter values to find the combination that yields the best results.\n",
    "\n",
    "There are several strategies for hyperparameter tuning, including:\n",
    "\n",
    "1. Manual search: This involves manually selecting hyperparameter values based on prior knowledge, intuition, or trial and error. It is a simple approach but can be time-consuming and may not yield the best results.\n",
    "2. Grid search: Grid search involves defining a grid of possible hyperparameter values and exhaustively searching through all combinations. It evaluates and compares models trained with each combination of hyperparameters. While it guarantees finding the best hyperparameter values within the defined grid, it can be computationally expensive when the search space is large.\n",
    "3. Random search: Random search selects hyperparameter values randomly from a predefined distribution. It performs a specified number of random searches and evaluates the models trained with each set of hyperparameters. Random search is more efficient than grid search when the search space is large and it doesn't require exhaustive evaluation of all combinations.\n",
    "4. Bayesian optimization: Bayesian optimization is an advanced technique that uses probabilistic models to predict the performance of different hyperparameter settings. It iteratively explores the search space by evaluating a few carefully selected points and updating the probabilistic model. Bayesian optimization can be more efficient than random search or grid search when the search space is large and expensive to evaluate.\n",
    "5. Automated techniques: There are automated hyperparameter tuning techniques like genetic algorithms, evolutionary algorithms, and reinforcement learning-based approaches that aim to optimize hyperparameters automatically. These techniques use optimization algorithms to search the hyperparameter space based on fitness evaluation.\n",
    "\n",
    "During hyperparameter tuning, it is common to use evaluation metrics such as accuracy, precision, recall, or mean squared error to assess the performance of the models trained with different hyperparameter values.\n",
    "\n",
    "By tuning hyperparameters effectively, you can improve the performance and generalization of your machine learning models, leading to better results on unseen data. Experiment tracking tools like TensorBoard can be valuable in this process, as they allow you to monitor and compare the performance of different hyperparameter settings over time.\n",
    "\n",
    "For this demonstration we will be using the simple but expensive grid search. In the later blogs I will show you how we can implement the other hyperparameter tuning algorithms. \n",
    "\n",
    "We will use the FashionMNIST dataset and tune the hyperparamters of our custom VGG model using Grid Search. \n",
    "\n",
    "Let's begin."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q torchinfo torchmetrics tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import os\n",
    "from torchvision.transforms import Resize, Compose, ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "from torchinfo import summary\n",
    "import torchmetrics\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "if not os.path.exists(\"data\"): os.mkdir(\"data\")\n",
    "\n",
    "train_transform = Compose([Resize((64,64)),\n",
    "                           ToTensor()\n",
    "                           ])\n",
    "test_transform = Compose([Resize((64,64)),\n",
    "                          ToTensor()\n",
    "                          ])\n",
    "\n",
    "\n",
    "training_dataset = torchvision.datasets.FashionMNIST(root = \"data\",\n",
    "                                                     download = True,\n",
    "                                                     train = True,\n",
    "                                                     transform = train_transform)\n",
    "\n",
    "test_dataset = torchvision.datasets.FashionMNIST(root = \"data\",\n",
    "                                                 download = True,\n",
    "                                                 train = False,\n",
    "                                                 transform = test_transform)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(training_dataset,\n",
    "                                          batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=2\n",
    "                                          )\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                              batch_size = BATCH_SIZE,\n",
    "                                              shuffle = False,\n",
    "                                              num_workers = 2\n",
    "                                              )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see how many examples are there in the training set and the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Images in test dataset is 10000\n",
      "Number of Images in training dataset is 60000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of Images in test dataset is {len(test_dataset)}\")\n",
    "print(f\"Number of Images in training dataset is {len(training_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1be925a4808>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(training_dataset[0][0].permute((1,2,0)), cmap = \"gray\")\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a TinyVGG Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class TinyVGG(nn.Module):\n",
    "    \"\"\"\n",
    "    A small VGG-like network for image classification.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): The number of input channels.\n",
    "        n_classes (int): The number of output classes.\n",
    "        hidden_units (int): The number of hidden units in each convolutional block.\n",
    "        n_conv_blocks (int): The number of convolutional blocks.\n",
    "        dropout (float): The dropout rate.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, n_classes, hidden_units, n_conv_blocks, dropout):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_features = n_classes\n",
    "        self.dropout = dropout\n",
    "        self.hidden_units = hidden_units\n",
    "\n",
    "        # Input block\n",
    "        self.input_block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=hidden_units, kernel_size=3, padding=0, stride=1),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Convolutional blocks\n",
    "        self.conv_blocks = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=3, padding=0, stride=1),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            ) for _ in range(n_conv_blocks)\n",
    "        ])\n",
    "\n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.LazyLinear(out_features=256),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(in_features=256, out_features=64),\n",
    "            nn.Linear(in_features=64, out_features=n_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor.\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.input_block(x)\n",
    "        for conv_block in self.conv_blocks:\n",
    "            x = conv_block(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "TinyVGG                                  [1, 10]                   --\n",
       "├─Sequential: 1-1                        [1, 10]                   --\n",
       "│    └─Sequential: 2-1                   [1, 64, 62, 62]           --\n",
       "│    │    └─Conv2d: 3-1                  [1, 64, 62, 62]           640\n",
       "│    │    └─Dropout: 3-2                 [1, 64, 62, 62]           --\n",
       "│    │    └─ReLU: 3-3                    [1, 64, 62, 62]           --\n",
       "│    └─Sequential: 2-2                   [1, 64, 30, 30]           --\n",
       "│    │    └─Conv2d: 3-4                  [1, 64, 60, 60]           36,928\n",
       "│    │    └─Dropout: 3-5                 [1, 64, 60, 60]           --\n",
       "│    │    └─ReLU: 3-6                    [1, 64, 60, 60]           --\n",
       "│    │    └─MaxPool2d: 3-7               [1, 64, 30, 30]           --\n",
       "│    └─Sequential: 2-3                   [1, 64, 14, 14]           (recursive)\n",
       "│    │    └─Conv2d: 3-8                  [1, 64, 28, 28]           (recursive)\n",
       "│    │    └─Dropout: 3-9                 [1, 64, 28, 28]           --\n",
       "│    │    └─ReLU: 3-10                   [1, 64, 28, 28]           --\n",
       "│    │    └─MaxPool2d: 3-11              [1, 64, 14, 14]           --\n",
       "│    └─Sequential: 2-4                   [1, 64, 6, 6]             (recursive)\n",
       "│    │    └─Conv2d: 3-12                 [1, 64, 12, 12]           (recursive)\n",
       "│    │    └─Dropout: 3-13                [1, 64, 12, 12]           --\n",
       "│    │    └─ReLU: 3-14                   [1, 64, 12, 12]           --\n",
       "│    │    └─MaxPool2d: 3-15              [1, 64, 6, 6]             --\n",
       "│    └─Sequential: 2-5                   [1, 10]                   --\n",
       "│    │    └─Flatten: 3-16                [1, 2304]                 --\n",
       "│    │    └─Linear: 3-17                 [1, 256]                  590,080\n",
       "│    │    └─Dropout: 3-18                [1, 256]                  --\n",
       "│    │    └─Linear: 3-19                 [1, 64]                   16,448\n",
       "│    │    └─Linear: 3-20                 [1, 10]                   650\n",
       "==========================================================================================\n",
       "Total params: 644,746\n",
       "Trainable params: 644,746\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 170.28\n",
       "==========================================================================================\n",
       "Input size (MB): 0.02\n",
       "Forward/backward pass size (MB): 4.29\n",
       "Params size (MB): 2.58\n",
       "Estimated Total Size (MB): 6.88\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model =  TinyVGG(in_channels = 1,\n",
    "                 n_classes = len(training_dataset.classes),\n",
    "                 hidden_units = 64,\n",
    "                 n_conv_blocks = 3,\n",
    "                 dropout = 0.1).to(device)\n",
    "summary(model, [1,64,64], batch_dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_step(dataloader, model, optimizer, criterion, device, train_acc_metric):\n",
    "    \"\"\"\n",
    "    Perform a single training step.\n",
    "\n",
    "    Args:\n",
    "        dataloader (torch.utils.data.DataLoader): The dataloader for the training data.\n",
    "        model (torch.nn.Module): The model to train.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer for the model.\n",
    "        criterion (torch.nn.Module): The loss function for the model.\n",
    "        device (torch.device): The device to train the model on.\n",
    "        train_acc_metric (torchmetrics.Accuracy): The accuracy metric for the model.\n",
    "\n",
    "    Returns:\n",
    "        The accuracy of the model on the training data.\n",
    "    \"\"\"\n",
    "\n",
    "    for (X, y) in tqdm.tqdm(dataloader):\n",
    "        # Move the data to the device.\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # Forward pass.\n",
    "        y_preds = model(X)\n",
    "\n",
    "        # Calculate the loss.\n",
    "        loss = criterion(y_preds, y)\n",
    "\n",
    "        # Calculate the accuracy.\n",
    "        train_acc_metric.update(y_preds, y)\n",
    "\n",
    "        # Backpropagate the loss.\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Zero the gradients.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    return train_acc_metric.compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(dataloader, model, device, test_acc_metric):\n",
    "    \"\"\"\n",
    "    Perform a single test step.\n",
    "\n",
    "    Args:\n",
    "        dataloader (torch.utils.data.DataLoader): The dataloader for the test data.\n",
    "        model (torch.nn.Module): The model to test.\n",
    "        device (torch.device): The device to test the model on.\n",
    "        test_acc_metric (torchmetrics.Accuracy): The accuracy metric for the model.\n",
    "\n",
    "    Returns:\n",
    "        The accuracy of the model on the test data.\n",
    "    \"\"\"\n",
    "\n",
    "    for (X, y) in tqdm.tqdm(dataloader):\n",
    "        # Move the data to the device.\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # Forward pass.\n",
    "        y_preds = model(X)\n",
    "\n",
    "        # Calculate the accuracy.\n",
    "        test_acc_metric.update(y_preds, y)\n",
    "\n",
    "    return test_acc_metric.compute()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = 0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, there are several hyperparameters as you can see - Learning Rate, Number of Epochs, type of optimizer, number of convolution layers, dropout and number of hidden units. We can first fix the learning rate and number of epoch and try to find the best number of convolution layers, dropout and hidden units. Once we have those we can then tune the number of epochs and learning rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def create_writer(\n",
    "    experiment_name: str, model_name: str, conv_layers, dropout, hidden_units\n",
    ") -> SummaryWriter:\n",
    "    \"\"\"\n",
    "    Create a SummaryWriter object for logging the training and test results.\n",
    "\n",
    "    Args:\n",
    "        experiment_name (str): The name of the experiment.\n",
    "        model_name (str): The name of the model.\n",
    "        conv_layers (int): The number of convolutional layers in the model.\n",
    "        dropout (float): The dropout rate used in the model.\n",
    "        hidden_units (int): The number of hidden units in the model.\n",
    "\n",
    "    Returns:\n",
    "        SummaryWriter: The SummaryWriter object.\n",
    "    \"\"\"\n",
    "\n",
    "    timestamp = str(datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\"))\n",
    "    log_dir = os.path.join(\n",
    "        \"runs\",\n",
    "        timestamp,\n",
    "        experiment_name,\n",
    "        model_name,\n",
    "        f\"{conv_layers}\",\n",
    "        f\"{dropout}\",\n",
    "        f\"{hidden_units}\",\n",
    "    ).replace(\"\\\\\", \"/\")\n",
    "    return SummaryWriter(log_dir=log_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Tuning Hyper Parameters || Conv Layers: 2 || Dropout: 0.1 || Hidden Units: 32 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:10<00:00, 89.13it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc = 0.8325999975204468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:02<00:00, 76.67it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc = 0.862500011920929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:10<00:00, 91.80it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc = 0.8614083528518677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:02<00:00, 77.59it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc = 0.8751999735832214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:10<00:00, 91.45it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc = 0.8762277960777283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:02<00:00, 75.34it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc = 0.8844333291053772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:10<00:00, 91.48it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc = 0.8858208060264587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:02<00:00, 78.04it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc = 0.8885250091552734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:10<00:00, 92.42it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc = 0.8926533460617065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:02<00:00, 77.00it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc = 0.8917199969291687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:10<00:00, 92.91it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc = 0.8980333209037781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:02<00:00, 76.99it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc = 0.8946499824523926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:10<00:00, 92.50it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc = 0.9025428295135498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:02<00:00, 76.34it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc = 0.8966857194900513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:10<00:00, 92.55it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc = 0.9064124822616577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:02<00:00, 77.62it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc = 0.8982250094413757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:10<00:00, 92.29it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc = 0.9097962975502014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:02<00:00, 77.96it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc = 0.8989889025688171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:10<00:00, 92.84it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc = 0.9127333164215088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:02<00:00, 77.66it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc = 0.8992199897766113\n",
      "\n",
      " Tuning Hyper Parameters || Conv Layers: 2 || Dropout: 0.1 || Hidden Units: 64 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:18<00:00, 49.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc = 0.8479166626930237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:02<00:00, 63.93it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc = 0.8844000101089478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:18<00:00, 50.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc = 0.8754249811172485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:02<00:00, 63.76it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc = 0.8926500082015991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:18<00:00, 50.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc = 0.8885111212730408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:02<00:00, 63.24it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc = 0.8987666964530945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:18<00:00, 49.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc = 0.8973166942596436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:02<00:00, 63.97it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc = 0.9006249904632568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:18<00:00, 49.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc = 0.9044533371925354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:02<00:00, 64.45it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc = 0.9036800265312195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:18<00:00, 50.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc = 0.9098583459854126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:02<00:00, 63.45it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc = 0.9048166871070862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:18<00:00, 50.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc = 0.9143785834312439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:02<00:00, 63.84it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc = 0.9058571457862854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:18<00:00, 50.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc = 0.9182083606719971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:02<00:00, 63.69it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc = 0.9064750075340271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:18<00:00, 50.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc = 0.921583354473114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:02<00:00, 63.52it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc = 0.9067777991294861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:18<00:00, 49.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc = 0.9245749711990356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:02<00:00, 63.73it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc = 0.9075700044631958\n",
      "\n",
      " Tuning Hyper Parameters || Conv Layers: 2 || Dropout: 0.1 || Hidden Units: 128 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:34<00:00, 27.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc = 0.840399980545044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:03<00:00, 46.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc = 0.885200023651123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:33<00:00, 27.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc = 0.8692416548728943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:03<00:00, 46.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc = 0.8912000060081482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:33<00:00, 27.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc = 0.8827278017997742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:03<00:00, 46.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc = 0.8953666687011719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 631/938 [00:23<00:10, 28.80it/s]"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This code performs hyperparameter tuning for a TinyVGG model.\n",
    "\n",
    "The hyperparameters that are tuned are the number of convolutional layers, the dropout rate, and the number of hidden units.\n",
    "\n",
    "The results of the hyperparameter tuning are logged to a TensorBoard file.\n",
    "\"\"\"\n",
    "\n",
    "experiment_number = 0\n",
    "\n",
    "# hyperparameters to tune\n",
    "hparams_config = {\n",
    "    \"n_conv_layers\": [1, 2, 3],\n",
    "    \"dropout\": [0.0, 0.25, 0.5],\n",
    "    \"hidden_units\": [128, 256, 512],\n",
    "}\n",
    "\n",
    "for n_conv_layers in hparams_config[\"n_conv_layers\"]:\n",
    "    for dropout in hparams_config[\"dropout\"]:\n",
    "        for hidden_units in hparams_config[\"hidden_units\"]:\n",
    "            experiment_number += 1\n",
    "            print(\n",
    "                f\"\\nTuning Hyper Parameters || Conv Layers: {n_conv_layers} || Dropout: {dropout} || Hidden Units: {hidden_units} \\n\"\n",
    "            )\n",
    "\n",
    "            # create the model\n",
    "            model = TinyVGG(\n",
    "                in_channels=1,\n",
    "                n_classes=len(training_dataset.classes),\n",
    "                hidden_units=hidden_units,\n",
    "                n_conv_blocks=n_conv_layers,\n",
    "                dropout=dropout,\n",
    "            ).to(device)\n",
    "\n",
    "            # create the optimizer and loss function\n",
    "            optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)\n",
    "            criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "            # create the accuracy metrics\n",
    "            train_acc_metric = torchmetrics.Accuracy(\n",
    "                task=\"multiclass\", num_classes=len(training_dataset.classes)\n",
    "            ).to(device)\n",
    "            test_acc_metric = torchmetrics.Accuracy(\n",
    "                task=\"multiclass\", num_classes=len(training_dataset.classes)\n",
    "            ).to(device)\n",
    "\n",
    "            # create the TensorBoard writer\n",
    "            writer = create_writer(\n",
    "                experiment_name=f\"{experiment_number}\",\n",
    "                model_name=\"tiny_vgg\",\n",
    "                conv_layers=n_conv_layers,\n",
    "                dropout=dropout,\n",
    "                hidden_units=hidden_units,\n",
    "            )\n",
    "\n",
    "            # train the model\n",
    "            for epoch in range(EPOCHS):\n",
    "                train_step(\n",
    "                    train_dataloader,\n",
    "                    model,\n",
    "                    optimizer,\n",
    "                    criterion,\n",
    "                    device,\n",
    "                    train_acc_metric,\n",
    "                )\n",
    "                test_step(test_dataloader, model, device, test_acc_metric)\n",
    "                writer.add_scalar(\n",
    "                    tag=\"Training Accuracy\",\n",
    "                    scalar_value=train_acc_metric.compute(),\n",
    "                    global_step=epoch,\n",
    "                )\n",
    "                writer.add_scalar(\n",
    "                    tag=\"Test Accuracy\",\n",
    "                    scalar_value=test_acc_metric.compute(),\n",
    "                    global_step=epoch,\n",
    "                )\n",
    "\n",
    "            # add the hyperparameters and metrics to TensorBoard\n",
    "            writer.add_hparams(\n",
    "                {\n",
    "                    \"conv_layers\": n_conv_layers,\n",
    "                    \"dropout\": dropout,\n",
    "                    \"hidden_units\": hidden_units,\n",
    "                },\n",
    "                {\n",
    "                    \"train_acc\": train_acc_metric.compute(),\n",
    "                    \"test_acc\": test_acc_metric.compute(),\n",
    "                },\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  TinyVGG(in_channels = 1,\n",
    "n_classes = len(training_dataset.classes),\n",
    "hidden_units = 32,\n",
    "n_conv_blocks = 2,\n",
    "dropout = 0.2).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = LEARNING_RATE)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_acc_metric = torchmetrics.Accuracy(task=\"multiclass\",\n",
    "                                    num_classes=len(training_dataset.classes)).to(device)\n",
    "test_acc_metric = torchmetrics.Accuracy(task=\"multiclass\",\n",
    "                                    num_classes=len(training_dataset.classes)).to(device)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
